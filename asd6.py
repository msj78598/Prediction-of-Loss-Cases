import os
import streamlit as st
import pandas as pd
import numpy as np
import joblib
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from io import BytesIO

# ===================== Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø© =====================
st.set_page_config(page_title="Ù†Ø¸Ø§Ù… ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙ‚Ø¯ Ø§Ù„Ø°ÙƒÙŠ", page_icon="âš¡", layout="wide")
os.environ["PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION"] = "python"

# ===================== Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… =====================
if os.name == "nt":
    model_folder = "C:\\asd6"
else:
    model_folder = "asd6"
os.makedirs(model_folder, exist_ok=True)

model_path = os.path.join(model_folder, 'ASD6.pkl')
data_frame_template_path = 'The data frame file to be analyzed.xlsx'

# ===================== ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ =====================
def train_and_save_model():
    try:
        file_path = 'final_classified_loss_with_reasons_60_percent_ordered.xlsx'
        if not os.path.exists(file_path):
            st.error("âš ï¸ Ù…Ù„Ù Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯! ÙŠØ±Ø¬Ù‰ Ø±ÙØ¹Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ.")
            return False

        data = pd.read_excel(file_path)
        X = data[['V1', 'V2', 'V3', 'A1', 'A2', 'A3']]
        y = data['Loss_Status'].apply(lambda x: 1 if x == 'Loss' else 0)

        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        model = RandomForestClassifier(
            n_estimators=500,
            max_depth=12,
            min_samples_split=4,
            class_weight="balanced",
            random_state=42
        )
        model.fit(X_train, y_train)
        joblib.dump(model, model_path)
        st.success("âœ… ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ­ÙØ¸Ù‡ Ø¨Ù†Ø¬Ø§Ø­!")
        return True
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {str(e)}")
        return False

if not os.path.exists(model_path):
    train_and_save_model()

# ===================== Ø«ÙˆØ§Ø¨Øª Ù‡Ù†Ø¯Ø³ÙŠØ© (Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ø¶Ø¨Ø· Ù…Ù† Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ) =====================
st.sidebar.header("âš™ï¸ Ø¹ØªØ¨Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„")
I_abs_min = st.sidebar.number_input("Ø­Ø¯ Ø§Ù„ØªÙŠØ§Ø± Ø§Ù„Ù…Ù‡Ù…Ù„ (A)", 0.1, 10.0, 0.5, 0.1)
I_rel_min = st.sidebar.slider("Ø­Ø¯ Ø§Ù„ØªÙŠØ§Ø± Ø§Ù„Ù†Ø³Ø¨ÙŠ Ù…Ù† Ø§Ù„Ù…ØªÙˆØ³Ø·", 0.0, 0.5, 0.10, 0.01)
pair_similarity = st.sidebar.slider("ØªÙ‚Ø§Ø±Ø¨ ØªÙŠØ§Ø±ÙŠ Ø§Ù„ÙØ§Ø²ØªÙŠÙ† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ØªÙŠÙ†", 0.05, 0.5, 0.18, 0.01)
V_zero_thr = st.sidebar.slider("Ø¹ØªØ¨Ø© Ø§Ù„Ø¬Ù‡Ø¯ â‰ˆ ØµÙØ± (V)", 0, 50, 20, 1)
V_ok_min = st.sidebar.slider("Ø£Ø¯Ù†Ù‰ Ø¬Ù‡Ø¯ Ø·Ø¨ÙŠØ¹ÙŠ (V)", 150, 240, 180, 1)
IUF_alarm = st.sidebar.slider("Ø­Ø¯ Ø¹Ø¯Ù… ØªÙˆØ§Ø²Ù† Ø§Ù„ØªÙŠØ§Ø± IUF", 0.1, 1.0, 0.40, 0.01)
VUF_alarm = st.sidebar.slider("Ø­Ø¯ Ø¹Ø¯Ù… ØªÙˆØ§Ø²Ù† Ø§Ù„Ø¬Ù‡Ø¯ (VUFâ‰ˆ)", 0.05, 0.5, 0.15, 0.01)

# ===================== Ø£Ø¯ÙˆØ§Øª Ù…Ø³Ø§Ø¹Ø¯Ø© =====================
def avg(x, eps=1e-9):
    return max(np.mean(x), eps)

def iuf(a1, a2, a3):
    A = np.array([a1, a2, a3], dtype=float)
    Abar = avg(A)
    return float(np.max(np.abs(A - Abar)) / Abar)

def vuf(v1, v2, v3):
    V = np.array([v1, v2, v3], dtype=float)
    Vmax, Vmin = float(np.max(V)), float(np.min(V))
    return 0.0 if Vmax == 0 else (Vmax - Vmin) / Vmax

# ===================== ÙƒØ´Ù Ø¹Ø¯Ø§Ø¯ Ø«Ù†Ø§Ø¦ÙŠ Ø§Ù„ÙØ§Ø² (ØµØ§Ø±Ù…) =====================
def detect_two_phase_strict(row):
    V = [float(row['V1']), float(row['V2']), float(row['V3'])]
    A = [float(row['A1']), float(row['A2']), float(row['A3'])]
    names = ['A1', 'A2', 'A3']

    # 1) Ø­Ø¯Ø¯ ÙØ§Ø²Ø© Ù…Ø±Ø´Ø­Ø© ÙƒÙ€ "ØºÙŠØ± Ù…Ø³ØªØ®Ø¯Ù…Ø©"
    candidates = []
    for i in range(3):
        # Ù…Ø±Ø´Ø­ Ø¥Ø°Ø§: (Vâ‰ˆ0) Ø£Ùˆ (Iâ‰ˆ0 Ù…Ø¹ V Ø·Ø¨ÙŠØ¹ÙŠ)
        cond_v_zero = V[i] <= V_zero_thr
        cond_i_zero = (A[i] <= max(I_abs_min, I_rel_min * avg([A[(i+1)%3], A[(i+2)%3]]))) and (V[i] >= V_ok_min)
        if cond_v_zero or cond_i_zero:
            candidates.append(i)

    # ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† ÙØ§Ø²Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø· Ù…Ø±Ø´Ø­Ø©
    if len(candidates) != 1:
        return (False, None, None, None)

    unused = candidates[0]
    used = [j for j in range(3) if j != unused]
    a_used = [A[j] for j in used]

    # 2) **Ø´Ø±Ø· Ù…Ø·Ù„Ù‚**: Ø§Ù„ÙØ§Ø²Ø© ØºÙŠØ± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© **ÙØ¹Ù„Ø§Ù‹** ØªÙŠØ§Ø±Ù‡Ø§ Ø¶Ø¹ÙŠÙ (Ù…Ø·Ù„Ù‚Ù‹Ø§ ÙˆÙ†ÙØ³Ø¨ÙŠÙ‹Ø§)
    A_unused_ok = A[unused] <= max(I_abs_min, I_rel_min * avg(a_used))

    # 3) Ø§Ù„ÙØ§Ø²ØªØ§Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ØªØ§Ù† Ù…ØªÙ‚Ø§Ø±Ø¨ØªØ§Ù† ÙˆØ¨Ù‚ÙŠÙ… Ù…Ø¹Ù‚ÙˆÙ„Ø©
    hi, lo = max(a_used), min(a_used)
    similar = (hi == 0 and lo == 0) or (hi > 0 and (hi - lo) / hi <= pair_similarity)
    strong_enough = (hi >= max(I_abs_min, I_rel_min * avg(a_used))) and (lo >= max(I_abs_min, I_rel_min * avg(a_used)))

    # 4) **Ø­Ø§Ù„Ø© Ø¥Ù„ØºØ§Ø¡ ØµØ±ÙŠØ­Ø©**: Ø¥Ù† ÙƒØ§Ù†Øª **Ø§Ù„ÙØ§Ø²Ø§Øª Ø§Ù„Ø«Ù„Ø§Ø«** ØªÙŠØ§Ø±Ø§ØªÙ‡Ø§ Ø¬Ù…ÙŠØ¹Ù‹Ø§ Ø£ÙƒØ¨Ø± Ù…Ù† Ø­Ø¯ Ù…Ø¹Ù‚ÙˆÙ„ â‡’ Ù„ÙŠØ³ Ø«Ù†Ø§Ø¦ÙŠ ÙØ§Ø²
    all_significant = all(x >= max(I_abs_min, I_rel_min * avg(A)) for x in A)

    if A_unused_ok and similar and strong_enough and (not all_significant):
        unused_reason = "Ø¬Ù‡Ø¯ ØºÙŠØ± Ù…ÙˆØµÙˆÙ„ (Vâ‰ˆ0)" if V[unused] <= V_zero_thr else "ØªÙŠØ§Ø± â‰ˆ ØµÙØ± Ù…Ø¹ Ø¬Ù‡Ø¯ Ø·Ø¨ÙŠØ¹ÙŠ"
        return (True, names[unused], f"{names[used[0]]}+{names[used[1]]}", unused_reason)

    return (False, None, None, None)

# ===================== Ø§Ù„ØªÙØ³ÙŠØ± Ø§Ù„Ù‡Ù†Ø¯Ø³ÙŠ =====================
def reason_engine(row):
    V1, V2, V3 = float(row['V1']), float(row['V2']), float(row['V3'])
    A1, A2, A3 = float(row['A1']), float(row['A2']), float(row['A3'])
    total_I = A1 + A2 + A3

    # Ø­Ù…ÙˆÙ„Ø© Ø¹Ø§Ù…Ø© Ù…Ù†Ø®ÙØ¶Ø© = Ù†ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø¶ÙˆØ¶Ø§Ø¡
    if total_I < 3 * I_abs_min:
        return "â„¹ï¸ ØªØ­Ù…ÙŠÙ„ Ù…Ù†Ø®ÙØ¶ (Ù„Ø§ Ø¥Ù†Ø°Ø§Ø±Ø§Øª)"

    # Ø¬Ù‡Ø¯ Ù…Ù†Ø®ÙØ¶ Ù…Ø¹ ØªÙŠØ§Ø±
    for p, Vp, Ap in [("A1",V1,A1),("A2",V2,A2),("A3",V3,A3)]:
        if Vp < 5 and Ap >= I_abs_min:
            return f"âš ï¸ Ø¬Ù‡Ø¯ Ù…Ù†Ø®ÙØ¶ Ø¬Ø¯Ù‹Ø§ Ù…Ø¹ ØªÙŠØ§Ø± Ø¹Ù„Ù‰ {p}"

    # Ø¹Ø¯Ù… ØªÙˆØ§Ø²Ù† Ø§Ù„Ø¬Ù‡Ø¯
    VUF = vuf(V1, V2, V3)
    if VUF >= VUF_alarm:
        return "âš ï¸ ÙØ±Ù‚ Ø¬Ù‡Ø¯ ÙƒØ¨ÙŠØ± Ø¨ÙŠÙ† Ø§Ù„ÙØ§Ø²Ø§Øª"

    # Ø¹Ø¯Ù… ØªÙˆØ§Ø²Ù† Ø§Ù„ØªÙŠØ§Ø±
    IUF = iuf(A1, A2, A3)
    if IUF >= IUF_alarm:
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙØ§Ø²Ø© Ø§Ù„Ù…Ø³ÙŠØ·Ø±Ø©
        mx = max(A1, A2, A3)
        p = ["A1","A2","A3"][[A1,A2,A3].index(mx)]
        return f"âš ï¸ Ø¹Ø¯Ù… ØªÙˆØ§Ø²Ù† ÙƒØ¨ÙŠØ± ÙÙŠ Ø§Ù„ØªÙŠØ§Ø±Ø§Øª (Ø§Ù„Ø£Ø¹Ù„Ù‰ {p})"

    return "âœ… Ù„Ø§ ØªÙˆØ¬Ø¯ Ø­Ø§Ù„Ø© ÙÙ‚Ø¯ Ù…Ø¤ÙƒØ¯Ø©"

# ===================== Ø­Ø³Ø§Ø¨ Ø§Ù„Ø®Ø·ÙˆØ±Ø© =====================
def severity_score(row):
    if row.get('Two_Phase_Meter', False):
        return 0.0
    V1, V2, V3 = float(row['V1']), float(row['V2']), float(row['V3'])
    A1, A2, A3 = float(row['A1']), float(row['A2']), float(row['A3'])
    s = 0.0
    s += 0.6 * iuf(A1, A2, A3)
    s += 0.3 * vuf(V1, V2, V3)
    lowVflag = int((V1 < 5 and A1 >= I_abs_min) or (V2 < 5 and A2 >= I_abs_min) or (V3 < 5 and A3 >= I_abs_min))
    s += 0.1 * lowVflag
    return round(min(max(s, 0.0), 1.0), 2)

def priority_label(s):
    if s >= 0.7: return "ğŸ”´ Ø¹Ø§Ù„ÙŠ Ø¬Ø¯Ù‹Ø§"
    if s >= 0.4: return "ğŸŸ  Ù…ØªÙˆØ³Ø·"
    if s >= 0.2: return "ğŸŸ¡ Ù…Ù†Ø®ÙØ¶"
    return "ğŸŸ¢ Ø³Ù„ÙŠÙ…"

# ===================== Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ =====================
def analyze_data(df):
    try:
        required = {'V1','V2','V3','A1','A2','A3','Meter Number'}
        missing = required - set(df.columns)
        if missing:
            st.error(f"âš ï¸ Ø§Ù„Ù…Ù„Ù ÙŠÙ†Ù‚ØµÙ‡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©: {', '.join(missing)}")
            return

        model = joblib.load(model_path)

        X = df[['V1','V2','V3','A1','A2','A3']]
        df = df.copy()
        df['Predicted_Loss'] = model.predict(X)

        # ÙƒØ§Ø´Ù Ø«Ù†Ø§Ø¦ÙŠ Ø§Ù„ÙØ§Ø² Ø§Ù„ØµØ§Ø±Ù…
        tp_cols = df.apply(
            lambda r: pd.Series(detect_two_phase_strict(r),
                                index=['Two_Phase_Meter','Unused_Phase','Two_Phase_Pair','Unused_Reason']),
            axis=1
        )
        df[['Two_Phase_Meter','Unused_Phase','Two_Phase_Pair','Unused_Reason']] = tp_cols

        # Ø§Ù„ØªÙØ³ÙŠØ± Ø§Ù„Ù‡Ù†Ø¯Ø³ÙŠ
        df['Reason'] = df.apply(reason_engine, axis=1)
        df.loc[df['Two_Phase_Meter'] == True, 'Reason'] = df.loc[df['Two_Phase_Meter'] == True].apply(
            lambda r: f"â„¹ï¸ Ø¹Ø¯Ø§Ø¯ Ø«Ù†Ø§Ø¦ÙŠ Ø§Ù„ÙØ§Ø² (Ù…Ø³ØªØ«Ù†Ù‰): ØºÙŠØ± Ù…Ø³ØªØ®Ø¯Ù… {r['Unused_Phase']} ({r['Unused_Reason']}), Ù…Ø³ØªØ®Ø¯Ù… {r['Two_Phase_Pair']}",
            axis=1
        )

        # Ø§Ù„Ø®Ø·ÙˆØ±Ø© ÙˆØ§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©
        df['Severity_Score'] = df.apply(severity_score, axis=1)
        df['Priority'] = df['Severity_Score'].apply(priority_label)

        # Ù†ÙˆØ¹ Ø§Ù„Ø­Ø§Ù„Ø©
        def classify_case(r):
            if r['Two_Phase_Meter']: return "â„¹ï¸ Ø¹Ø¯Ø§Ø¯ Ø«Ù†Ø§Ø¦ÙŠ Ø§Ù„ÙØ§Ø² (Ù…Ø³ØªØ«Ù†Ù‰)"
            if r['Predicted_Loss']==1 and "âš ï¸" in r['Reason']: return "ğŸ“Š ÙØ§Ù‚Ø¯ Ù…Ø¤ÙƒØ¯ (Ù†Ù…ÙˆØ°Ø¬+Ù…Ø­Ø¯Ø¯Ø§Øª)"
            if r['Predicted_Loss']==1: return "ğŸ¤– ÙØ§Ù‚Ø¯ (Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙ‚Ø·)"
            if r['Predicted_Loss']==0 and "âš ï¸" in r['Reason']: return "ğŸ§  ÙØ§Ù‚Ø¯ Ù‡Ù†Ø¯Ø³ÙŠ ÙÙ‚Ø·"
            return "âœ… Ø³Ù„ÙŠÙ…"
        df['Case_Type'] = df.apply(classify_case, axis=1)

        # ===== Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø± Ù„ÙƒÙ„ Ø¹Ø¯Ø§Ø¯ (Ù†Ø£Ø®Ø° Ø§Ù„Ø£Ø¹Ù„Ù‰ Ø®Ø·ÙˆØ±Ø©) =====
        df = df.sort_values(['Meter Number','Severity_Score'], ascending=[True, False])\
               .drop_duplicates(subset=['Meter Number'], keep='first')

        # Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¹Ø±Ø¶
        detected_loss = df[df['Predicted_Loss'] == 1]
        high_priority = detected_loss[detected_loss['Reason'].str.contains('âš ï¸')]
        logical_only = df[df['Case_Type'].str.contains("Ù‡Ù†Ø¯Ø³ÙŠ")]
        two_phase_only = df[df['Two_Phase_Meter'] == True]
        high_impact = df[(df['Severity_Score'] >= 0.7) & (~df['Two_Phase_Meter'])]\
                        .sort_values(by='Severity_Score', ascending=False)

        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        total = len(df)
        tp_count = len(two_phase_only)
        effective_total = max(total - tp_count, 1)
        detected_count = len(detected_loss)
        logical_count = len(logical_only)
        high_impact_count = len(high_impact)
        loss_ratio = round(((detected_count + logical_count) / effective_total) * 100, 2)

        st.markdown("## ğŸ“ˆ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„")
        c1, c2, c3, c4, c5, c6 = st.columns(6)
        c1.metric("Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ø¯Ø§Ø¯Ø§Øª", total)
        c2.metric("Ø«Ù†Ø§Ø¦ÙŠ ÙØ§Ø² (Ù…Ø³ØªØ«Ù†Ù‰)", tp_count)
        c3.metric("ÙØ§Ù‚Ø¯ Ø¨Ø§Ù„Ù†Ù…ÙˆØ°Ø¬", detected_count)
        c4.metric("ÙØ§Ù‚Ø¯ Ù‡Ù†Ø¯Ø³ÙŠ ÙÙ‚Ø·", logical_count)
        c5.metric("Ø¹Ø§Ù„ÙŠ Ø§Ù„ØªØ£Ø«ÙŠØ±", high_impact_count)
        c6.metric("Ù†Ø³Ø¨Ø© Ø§Ù„ÙÙ‚Ø¯ (Ø¨Ø¹Ø¯ Ø§Ù„Ø§Ø³ØªØ«Ù†Ø§Ø¡)", f"{loss_ratio}%")

        st.markdown("---")
        st.subheader("ğŸ“Š Ø¬Ù…ÙŠØ¹ Ø­Ø§Ù„Ø§Øª Ø§Ù„ÙÙ‚Ø¯ Ø§Ù„Ù…ÙƒØªØ´ÙØ© Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬")
        st.dataframe(detected_loss)

        st.subheader("ğŸš¨ Ø­Ø§Ù„Ø§Øª Ø§Ù„ÙÙ‚Ø¯ Ø°Ø§Øª Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ø§Ù„Ø¹Ø§Ù„ÙŠØ©")
        st.dataframe(high_priority)

        st.subheader("ğŸ§  Ø­Ø§Ù„Ø§Øª ØªÙ†Ø·Ø¨Ù‚ Ø¹Ù„ÙŠÙ‡Ø§ Ø§Ù„Ù…Ø­Ø¯Ø¯Ø§Øª ÙˆÙ„Ù… ÙŠÙƒØªØ´ÙÙ‡Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬")
        st.dataframe(logical_only)

        st.subheader("ğŸ”´ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø£Ø¹Ù„Ù‰ ØªØ£Ø«ÙŠØ±Ù‹Ø§ ÙˆØ®Ø·ÙˆØ±Ø©")
        st.dataframe(high_impact)

        st.subheader("â„¹ï¸ Ø¹Ø¯Ø§Ø¯Ø§Øª Ø«Ù†Ø§Ø¦ÙŠØ© Ø§Ù„ÙØ§Ø² (Ù…Ø³ØªØ«Ù†Ø§Ø©)")
        st.dataframe(two_phase_only)

        # ØªÙ‚Ø±ÙŠØ± Excel Ø¨Ù†Ø²Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±
        output = BytesIO()
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            detected_loss.to_excel(writer, sheet_name="Model_Detected", index=False)
            high_priority.to_excel(writer, sheet_name="High_Priority", index=False)
            logical_only.to_excel(writer, sheet_name="Logical_Only", index=False)
            high_impact.to_excel(writer, sheet_name="High_Impact", index=False)
            two_phase_only.to_excel(writer, sheet_name="Two_Phase_Meters", index=False)
        output.seek(0)

        st.download_button(
            "ğŸ“¥ ØªØ­Ù…ÙŠÙ„ ØªÙ‚Ø±ÙŠØ± Ø´Ø§Ù…Ù„ (Excel)",
            data=output,
            file_name="loss_analysis_report.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )

    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {str(e)}")

# ===================== ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ =====================
st.sidebar.title("ğŸ§ª Ø§Ù„ØªØ­Ù„ÙŠÙ„")
st.title("âš¡ Ù†Ø¸Ø§Ù… Ø°ÙƒÙŠ Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØ§ÙƒØªØ´Ø§Ù Ø­Ø§Ù„Ø§Øª Ø§Ù„ÙÙ‚Ø¯")
st.markdown("ÙŠØ¶Ø¨Ø· Ø§Ù„Ø¹ØªØ¨Ø§Øª Ù…Ù† Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ ÙˆÙŠÙ…Ù†Ø¹ ØªØµÙ†ÙŠÙ Ø«Ù†Ø§Ø¦ÙŠ ÙØ§Ø² Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ø­Ù…Ù„ Ù…Ù„Ø­ÙˆØ¸ Ø¹Ù„Ù‰ Ø§Ù„ÙØ§Ø²Ø§Øª Ø§Ù„Ø«Ù„Ø§Ø«.")

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù„Ø¨
if os.path.exists(data_frame_template_path):
    with open(data_frame_template_path, 'rb') as f:
        st.sidebar.download_button("ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª", data=f.read(),
                                   file_name="The_data_frame_file_to_be_analyzed.xlsx")
else:
    st.sidebar.warning("âš ï¸ Ù‚Ø§Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ØªÙˆÙØ±!")

# Ø±ÙØ¹ ÙˆØªØ­Ù„ÙŠÙ„
st.header("ğŸ“¤ ØªØ­Ù„ÙŠÙ„ Ù…Ù„Ù Excel")
file = st.file_uploader("Ø§Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù", type=["xlsx"])
if file is not None:
    try:
        df = pd.read_excel(file)
        analyze_data(df)
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù: {str(e)}")

st.markdown("---")
st.markdown("ğŸ‘¨â€ğŸ’» Ø§Ù„Ù…Ø·ÙˆØ±: Ù…Ø´Ù‡ÙˆØ± Ø§Ù„Ø¹Ø¨Ø§Ø³ â€” Ø¢Ø®Ø± ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙŠÙˆÙ…")
